{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.jianshu.com/p/2ccbab48414b\n",
    "#https://github.com/princewen/tensorflow_practice/tree/master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# reproducible\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "class PolicyGradient:\n",
    "    def __init__(self,\n",
    "                 n_actions,\n",
    "                 n_features,\n",
    "                 learning_rate = 0.01,\n",
    "                 reward_decay = 0.9,\n",
    "                 output_graph = False):\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "\n",
    "        self.ep_obs,self.ep_as,self.ep_rs = [],[],[]\n",
    "\n",
    "        self._build_net()\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        if output_graph:\n",
    "            tf.summary.FileWriter(\"logs/\",self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.tf_obs = tf.placeholder(tf.float32,[None,self.n_features],name='observation')\n",
    "            self.tf_acts = tf.placeholder(tf.int32,[None,],name='actions_num')\n",
    "            self.tf_vt = tf.placeholder(tf.float32,[None,],name='actions_value')\n",
    "\n",
    "        layer = tf.layers.dense(\n",
    "            inputs = self.tf_obs,\n",
    "            units = 10,\n",
    "            activation= tf.nn.tanh,\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0,stddev=0.3),\n",
    "            bias_initializer= tf.constant_initializer(0.1),\n",
    "            name='fc1'\n",
    "        )\n",
    "\n",
    "        all_act = tf.layers.dense(\n",
    "            inputs = layer,\n",
    "            units = self.n_actions,\n",
    "            activation = None,\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0,stddev=0.3),\n",
    "            bias_initializer = tf.constant_initializer(0.1),\n",
    "            name='fc2'\n",
    "        )\n",
    "\n",
    "        self.all_act_prob = tf.nn.softmax(all_act,name='act_prob')\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            #neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.all_act_prob,labels =self.tf_acts)\n",
    "\n",
    "            neg_log_prob = tf.reduce_sum(-tf.log(self.all_act_prob) * tf.one_hot(indices=self.tf_acts,depth=self.n_actions),axis=1)\n",
    "            loss = tf.reduce_mean(neg_log_prob * self.tf_vt)\n",
    "            self.neg_log_prob = neg_log_prob\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)\n",
    "\n",
    "\n",
    "\n",
    "    def choose_action(self,observation):\n",
    "        prob_weights = self.sess.run(self.all_act_prob,feed_dict={self.tf_obs:observation[np.newaxis,:]})\n",
    "        action = np.random.choice(range(prob_weights.shape[1]),p=prob_weights.ravel())\n",
    "        return action\n",
    "\n",
    "\n",
    "    def store_transition(self,s,a,r):\n",
    "        self.ep_obs.append(s)\n",
    "        self.ep_as.append(a)\n",
    "        self.ep_rs.append(r)\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        discounted_ep_rs_norm = self._discount_and_norm_rewards()\n",
    "\n",
    "        print(self.sess.run([self.train_op,self.all_act_prob,self.neg_log_prob],feed_dict={\n",
    "            self.tf_obs:np.vstack(self.ep_obs),\n",
    "            self.tf_acts:np.array(self.ep_as),\n",
    "            self.tf_vt:discounted_ep_rs_norm,\n",
    "        }))\n",
    "        #print(\"self.all_act_prob\",self.all_act_prob)\n",
    "        #print(\"self.neg_log_prob\",self.neg_log_prob)\n",
    "       \n",
    "        print(\"discounted_ep_rs_norm\",discounted_ep_rs_norm)\n",
    "        self.ep_obs,self.ep_as,self.ep_rs = [],[],[]\n",
    "        return discounted_ep_rs_norm\n",
    "\n",
    "\n",
    "\n",
    "    def _discount_and_norm_rewards(self):\n",
    "        discounted_ep_rs = np.zeros_like(self.ep_rs)\n",
    "        running_add = 0\n",
    "        print(\"self.ep_rs\",self.ep_rs)\n",
    "        # reserved 返回的是列表的反序，这样就得到了贴现求和值。\n",
    "        for t in reversed(range(0,len(self.ep_rs))):\n",
    "            running_add = running_add * self.gamma + self.ep_rs[t]\n",
    "            discounted_ep_rs[t] = running_add\n",
    "        \n",
    "        discounted_ep_rs -= np.mean(discounted_ep_rs)\n",
    "        discounted_ep_rs /= np.std(discounted_ep_rs)\n",
    "        return discounted_ep_rs\n",
    "\n",
    "\n",
    "import gym\n",
    "#from RL_brain import PolicyGradient\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DISPLAY_REWARD_THRESHOLD = 400\n",
    "RENDER = False\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(1)\n",
    "env = env.unwrapped\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "n_features = env.observation_space.shape[0]\n",
    "\n",
    "RL = PolicyGradient(\n",
    "    n_actions=n_actions,\n",
    "    n_features = n_features,\n",
    "    learning_rate = 0.02,\n",
    "    reward_decay = 0.99\n",
    ")\n",
    "\n",
    "for i_episode in range(3000):\n",
    "    observation = env.reset()\n",
    "\n",
    "    while True:\n",
    "        if RENDER:env.render()\n",
    "        action = RL.choose_action(observation)\n",
    "\n",
    "        observation_,reward,done,info = env.step(action)\n",
    "\n",
    "        RL.store_transition(observation,action,reward)\n",
    "\n",
    "\n",
    "        if done:\n",
    "            #print(RL.ep_rs)\n",
    "            ep_rs_sum = sum(RL.ep_rs)\n",
    "\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.99 + ep_rs_sum * 0.01\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True     # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "\n",
    "            vt = RL.learn()\n",
    "            #print(\"neg_log_prob\",RL.neg_log_prob)\n",
    "            #print(\"self.tf_vt\",RL.tf_vt)\n",
    "            if i_episode == 0:\n",
    "                plt.plot(vt)    # plot the episode vt\n",
    "                plt.xlabel('episode steps')\n",
    "                plt.ylabel('normalized state-action value')\n",
    "                plt.show()\n",
    "            break\n",
    "\n",
    "        observation = observation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
