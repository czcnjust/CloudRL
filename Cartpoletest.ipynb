{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "better parameters:\n",
      "[-0.60998024 -0.33600103  0.93460941  0.85103124]\n",
      "bestreward:\n",
      "178.0\n",
      "better parameters:\n",
      "[0.54676226 0.0980618  0.94049795 0.99577082]\n",
      "bestreward:\n",
      "200.0\n",
      "Hill climbing:#######################\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "better parameters:\n",
      "[-0.62657577 -1.0338177   0.9212405   0.24599501]\n",
      "bestreward:\n",
      "87.0\n",
      "better parameters:\n",
      "[-0.52862169 -1.05704701  1.0004506   0.30510291]\n",
      "bestreward:\n",
      "94.0\n",
      "better parameters:\n",
      "[-0.59276638 -0.98431025  1.05937998  0.38799486]\n",
      "bestreward:\n",
      "103.0\n",
      "better parameters:\n",
      "[-0.60155914 -0.90372944  1.02527021  0.4259723 ]\n",
      "bestreward:\n",
      "109.0\n",
      "better parameters:\n",
      "[-0.63487944 -0.80876368  0.97310605  0.52534324]\n",
      "bestreward:\n",
      "200.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "def run_episode(env, parameters):  \n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(parameters,observation) < 0 else 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "def randomsearch(env):\n",
    "    bestparams = None  \n",
    "    bestreward = 0  \n",
    "    for _ in range(10000):  \n",
    "        parameters = np.random.rand(4) * 2 - 1\n",
    "        reward = run_episode(env,parameters)\n",
    "        if reward > bestreward:\n",
    "            bestreward = reward\n",
    "            bestparams = parameters\n",
    "            print(\"better parameters:\")\n",
    "            print(bestparams)\n",
    "            print(\"bestreward:\")\n",
    "            print(bestreward)\n",
    "            # considered solved if the agent lasts 200 timesteps\n",
    "            if reward == 200:\n",
    "                break\n",
    "def hillclimbing(env):\n",
    "    noise_scaling = 0.1  \n",
    "    parameters = np.random.rand(4) * 2 - 1  \n",
    "    bestreward = 0  \n",
    "    for _ in range(10000):  \n",
    "        newparams = parameters + (np.random.rand(4) * 2 - 1)*noise_scaling\n",
    "        reward = 0\n",
    "        reward = run_episode(env,newparams)\n",
    "        \n",
    "        if reward > bestreward:\n",
    "            bestreward = reward\n",
    "            parameters = newparams\n",
    "            print(\"better parameters:\")\n",
    "            print(parameters)\n",
    "            print(\"bestreward:\")\n",
    "            print(bestreward)\n",
    "        if reward == 200:\n",
    "            break\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    randomsearch(env)\n",
    "    #env.reset()\n",
    "    print(\"Hill climbing:#######################\")\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    hillclimbing(env)\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://chenrudan.github.io/blog/2016/09/04/cartpole.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://chenrudan.github.io/blog/2016/08/03/reinforcementlearninglesssion7.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "weight [ 1.09364854 -0.76305567  0.67415771  2.83642659]\n",
      "Monte-Carlo policy gradient get reward 180.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "def generate_episode(env, weight):\n",
    "    episode = []\n",
    "    pre_observation = env.reset()\n",
    "\n",
    "    t = 0\n",
    "    #generate 1 episodes for training.\n",
    "    while 1:\n",
    "        #env.render()\n",
    "        pi, action = choose_action(weight, pre_observation)\n",
    "    \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        episode.append([pre_observation, action, pi, reward])\n",
    "        pre_observation = observation\n",
    "    \n",
    "        t += 1\n",
    "        if done or t > 1000:\n",
    "            break\n",
    "    return episode\n",
    "\n",
    "def evaluate_given_parameter_sigmoid(env, weight):\n",
    "    observation = env.reset()\n",
    "    total_reward = 0.\n",
    "    for t in range(1000):\n",
    "        env.render()\n",
    "        weighted_sum = np.dot(weight, observation)\n",
    "        pi = 1 / (1 + np.exp(-weighted_sum))\n",
    "        if pi > 0.5:\n",
    "            action = 1\n",
    "        else:\n",
    "            action = 0\n",
    "        time.sleep(0.4)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def monte_carlo_policy_gradient(env):\n",
    "\n",
    "    learning_rate = -0.0001\n",
    "    best_reward = -100.0\n",
    "\n",
    "    weight = np.random.rand(4)\n",
    "\n",
    "    for iiter in range(2000):\n",
    "\n",
    "        cur_episode = generate_episode(env, weight)\n",
    "        for t in range(len(cur_episode)):\n",
    "             \n",
    "            observation, action, pi, reward = cur_episode[t]\n",
    "            #print(observation, action, pi, reward)\n",
    "            #weight += learning_rate*(1-pi)*np.transpose(-observation)*reward\n",
    "            #update weight if πθ(s,1)=1/(1+e−wx)-0.5 sigmoid, ▽θlogπθ(s,1)=(1−pi)∗(−x) \n",
    "            #πθ(s,0)=0.5-1/(1+e−wx), ▽θlogπθ(s,1)=(-1)(1−pi)∗(−x)\n",
    "            #improved by czc\n",
    "            if pi > 0.5:\n",
    "                weight += learning_rate*(1-pi)*np.transpose(-observation)*reward\n",
    "            else:\n",
    "                weight += learning_rate*(-1)*(1-pi)*np.transpose(-observation)*reward\n",
    "            \n",
    "    print ('weight', weight) \n",
    "    cur_reward = evaluate_given_parameter_sigmoid(env, weight)\n",
    "    print ('Monte-Carlo policy gradient get reward', cur_reward)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
    "#测试结果\n",
    "scores = [3.0,1.0, 0.2]\n",
    "print softmax(scores)\n",
    "def choose_actionbysoftmax(weight, observation):\n",
    "\n",
    "    weighted_sum1 = np.dot(weight[4,0], observation)\n",
    "    weighted_sum2 = np.dot(weight[4,1], observation)\n",
    "    epower=softmax([weighted_sum1,weighted_sum2])\n",
    "    \n",
    "    pi = 1 / (1 + np.exp(-weighted_sum))#sigmoid function \n",
    "    if pi > 0.5:\n",
    "        action = 1\n",
    "    else:\n",
    "        action = 0\n",
    "    return pi, action\n",
    "\n",
    "def choose_action(weight, observation):\n",
    "\n",
    "    weighted_sum = np.dot(weight, observation)\n",
    "    pi = 1 / (1 + np.exp(-weighted_sum))#sigmoid function \n",
    "    if pi > 0.5:\n",
    "        action = 1\n",
    "    else:\n",
    "        action = 0\n",
    "    return pi, action\n",
    "# \n",
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "\n",
    "def actor_critic_policy_gradientDeepQN(env):\n",
    "  \n",
    "   \n",
    "    gamma = 1\n",
    "\n",
    "    p_weight = np.random.rand(4，2)\n",
    "        \n",
    "    #weight for value function\n",
    "    v_weight = np.random.rand(4)\n",
    "\n",
    "    p_learning_rate = -0.0001\n",
    "    v_learning_rate = -0.0001\n",
    "\n",
    "    done = True\n",
    "\n",
    "    for iiter in range(1000):\n",
    "\n",
    "        t = 0\n",
    "        while 1:\n",
    "            if done:\n",
    "                #print ('start new training...')\n",
    "                #print ('p_weight', p_weight)\n",
    "                #print ('v_weight', v_weight)\n",
    "\n",
    "                pre_observation = env.reset()\n",
    "                pre_pi, pre_action = choose_action(p_weight, pre_observation)\n",
    "        \n",
    "                pre_phi = pre_observation\n",
    "                pre_q = np.dot(v_weight, pre_phi)\n",
    "\n",
    "            #env.render()\n",
    "\n",
    "            observation, reward, done, info = env.step(pre_action)\n",
    "\n",
    "            pi, action = choose_action(p_weight, observation)\n",
    "            \n",
    "            phi = observation\n",
    "            q = np.dot(v_weight, phi)\n",
    "\n",
    "            delta = reward + gamma*q - pre_q\n",
    "            p_weight += p_learning_rate*(1-pre_pi)*np.transpose(-pre_observation)*pre_q\n",
    "            #if pi > 0.5:\n",
    "            #    p_weight += p_learning_rate*(1-pre_pi)*np.transpose(-pre_observation)*pre_q\n",
    "            #else:\n",
    "            #    p_weight += p_learning_rate*(-1)*(1-pre_pi)*np.transpose(-pre_observation)*pre_q\n",
    "            \n",
    "\n",
    "            v_weight += v_learning_rate*delta*np.transpose(pre_phi)\n",
    "\n",
    "            pre_pi = pi\n",
    "            pre_observation = observation\n",
    "            pre_q = q\n",
    "            pre_phi = phi\n",
    "            pre_action = action\n",
    "\n",
    "            t += 1\n",
    "            if done:\n",
    "                break\n",
    "    print ('p_weight', p_weight)\n",
    "    print ('v_weight', v_weight)\n",
    "    cur_reward = evaluate_given_parameter_sigmoid(env, p_weight)\n",
    "    print ('Actor critic policy gradient get reward', cur_reward)\n",
    "    def actor_critic_policy_gradient(env):\n",
    "    gamma = 1\n",
    "\n",
    "    p_weight = np.random.rand(4)\n",
    "        \n",
    "    #weight for value function\n",
    "    v_weight = np.random.rand(4)\n",
    "\n",
    "    p_learning_rate = -0.0001\n",
    "    v_learning_rate = -0.0001\n",
    "\n",
    "    done = True\n",
    "\n",
    "    for iiter in range(1000):\n",
    "\n",
    "        t = 0\n",
    "        while 1:\n",
    "            if done:\n",
    "                #print ('start new training...')\n",
    "                #print ('p_weight', p_weight)\n",
    "                #print ('v_weight', v_weight)\n",
    "\n",
    "                pre_observation = env.reset()\n",
    "                pre_pi, pre_action = choose_action(p_weight, pre_observation)\n",
    "        \n",
    "                pre_phi = pre_observation\n",
    "                pre_q = np.dot(v_weight, pre_phi)\n",
    "\n",
    "            #env.render()\n",
    "\n",
    "            observation, reward, done, info = env.step(pre_action)\n",
    "\n",
    "            pi, action = choose_action(p_weight, observation)\n",
    "            \n",
    "            phi = observation\n",
    "            q = np.dot(v_weight, phi)\n",
    "\n",
    "            delta = reward + gamma*q - pre_q\n",
    "            p_weight += p_learning_rate*(1-pre_pi)*np.transpose(-pre_observation)*pre_q\n",
    "            #if pi > 0.5:\n",
    "            #    p_weight += p_learning_rate*(1-pre_pi)*np.transpose(-pre_observation)*pre_q\n",
    "            #else:\n",
    "            #    p_weight += p_learning_rate*(-1)*(1-pre_pi)*np.transpose(-pre_observation)*pre_q\n",
    "            \n",
    "\n",
    "            v_weight += v_learning_rate*delta*np.transpose(pre_phi)\n",
    "\n",
    "            pre_pi = pi\n",
    "            pre_observation = observation\n",
    "            pre_q = q\n",
    "            pre_phi = phi\n",
    "            pre_action = action\n",
    "\n",
    "            t += 1\n",
    "            if done:\n",
    "                break\n",
    "    print ('p_weight', p_weight)\n",
    "    print ('v_weight', v_weight)\n",
    "    cur_reward = evaluate_given_parameter_sigmoid(env, p_weight)\n",
    "    print ('Actor critic policy gradient get reward', cur_reward)\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "#env.monitor.start('cartpole-hill/', force=True)\n",
    "#actor_critic_policy_gradient(env)\n",
    "#env.monitor.close()\n",
    "\n",
    "monte_carlo_policy_gradient(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://www.jianshu.com/p/2ccbab48414b\n",
    "#https://github.com/princewen/tensorflow_practice/tree/master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1855881  0.82261147 0.96411283 0.73660345]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def printa():\n",
    "    a=np.random.rand(4)\n",
    "    print(a)\n",
    "printa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
